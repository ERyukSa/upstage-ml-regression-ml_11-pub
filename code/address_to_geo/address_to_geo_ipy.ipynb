{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel coordinate processing...\n",
      "Initial missing coordinates: 6562\n",
      "Using 63 CPU cores for parallel processing\n",
      "Split data into 657 chunks for processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 657/657 [00:02<00:00, 318.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated 6562 results. Updating dataframe...\n",
      "\n",
      "Processing completed in 4.13 seconds\n",
      "Successfully filled 6557 out of 6562 missing coordinates\n",
      "Remaining missing coordinates: 146\n",
      "Match methods statistics:\n",
      "  - exact_match: 6557\n",
      "  - apartment_match: 0\n",
      "  - neighborhood_match: 0\n",
      "Updated dataset saved to updated_test_with_coordinates_parallel.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the dataset and the address to geo mapping\"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv('../data/test.csv', encoding='utf-8', low_memory=False)\n",
    "    \n",
    "    # Load address to geo coordinates mapping\n",
    "    with open('adres_to_geo.pickle', 'rb') as f:\n",
    "        adres_to_geo = pickle.load(f)\n",
    "    \n",
    "    return df, adres_to_geo\n",
    "\n",
    "def create_address_keys(row, first_col_name):\n",
    "    \"\"\"Create multiple possible address keys to maximize matching chances\"\"\"\n",
    "    keys = []\n",
    "    \n",
    "    # Extract components from the first column (assuming it contains the address)\n",
    "    try:\n",
    "        base_address = row[first_col_name]  # Get the first column value\n",
    "        if pd.isna(base_address):\n",
    "            return keys\n",
    "            \n",
    "        # Strategy 1: Full address with original 본번/부번 format\n",
    "        try:\n",
    "            if pd.notna(row['본번']):\n",
    "                if pd.notna(row['부번']) and row['부번'] != 0:\n",
    "                    plot_number = f\"{int(row['본번'])}-{int(row['부번'])}\"\n",
    "                else:\n",
    "                    plot_number = str(int(row['본번']))\n",
    "                \n",
    "                full_address = f\"{base_address} {plot_number}\"\n",
    "                keys.append(full_address)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Strategy 2: Address parts with original 본번/부번 format\n",
    "        # Extract city, district, neighborhood if they exist in the address\n",
    "        try:\n",
    "            address_parts = base_address.split()\n",
    "            if len(address_parts) >= 3:  # Ensure we have at least city+district+neighborhood\n",
    "                neighborhood = address_parts[2]  # e.g., \"개포동\"\n",
    "                district = address_parts[1]      # e.g., \"강남구\"\n",
    "                city = address_parts[0]          # e.g., \"서울특별시\"\n",
    "                \n",
    "                # Different combinations\n",
    "                if pd.notna(row['본번']):\n",
    "                    if pd.notna(row['부번']) and row['부번'] != 0:\n",
    "                        plot_number = f\"{int(row['본번'])}-{int(row['부번'])}\"\n",
    "                    else:\n",
    "                        plot_number = str(int(row['본번']))\n",
    "                    \n",
    "                    keys.append(f\"{city} {district} {neighborhood} {plot_number}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Strategy 3: Try with just the neighborhood and 번지\n",
    "        try:\n",
    "            if len(address_parts) >= 3 and pd.notna(row['본번']):\n",
    "                neighborhood = address_parts[2]  # e.g., \"개포동\"\n",
    "                \n",
    "                if pd.notna(row['부번']) and row['부번'] != 0:\n",
    "                    plot_number = f\"{int(row['본번'])}-{int(row['부번'])}\"\n",
    "                else:\n",
    "                    plot_number = str(int(row['본번']))\n",
    "                \n",
    "                keys.append(f\"{neighborhood} {plot_number}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Strategy 4: Extract address using regular expressions\n",
    "        try:\n",
    "            match = re.search(r'(서울[특별시]*|인천[광역시]*|부산[광역시]*|대구[광역시]*|광주[광역시]*|대전[광역시]*|울산[광역시]*|세종[특별자치시]*|경기도|강원도|충청북도|충청남도|전라북도|전라남도|경상북도|경상남도|제주[특별자치도]*)\\s+([^\\s]+)\\s+([^\\s]+)', base_address)\n",
    "            if match:\n",
    "                city = match.group(1)\n",
    "                district = match.group(2)\n",
    "                neighborhood = match.group(3)\n",
    "                \n",
    "                if pd.notna(row['본번']):\n",
    "                    if pd.notna(row['부번']) and row['부번'] != 0:\n",
    "                        plot_number = f\"{int(row['본번'])}-{int(row['부번'])}\"\n",
    "                    else:\n",
    "                        plot_number = str(int(row['본번']))\n",
    "                    \n",
    "                    keys.append(f\"{city} {district} {neighborhood} {plot_number}\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def process_chunk(chunk_data):\n",
    "    \"\"\"Process a chunk of data in parallel\"\"\"\n",
    "    chunk_df, adres_to_geo, first_col_name = chunk_data\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in chunk_df.iterrows():\n",
    "        coord_x = row['좌표X']\n",
    "        coord_y = row['좌표Y']\n",
    "        \n",
    "        # Skip if coordinates are already filled\n",
    "        if pd.notna(coord_x) and pd.notna(coord_y):\n",
    "            results.append((idx, coord_x, coord_y, None))\n",
    "            continue\n",
    "        \n",
    "        # Try all key strategies\n",
    "        possible_keys = create_address_keys(row, first_col_name)\n",
    "        match_method = None\n",
    "        \n",
    "        # Try direct matches first\n",
    "        for key in possible_keys:\n",
    "            if key in adres_to_geo:\n",
    "                latitude, longitude = adres_to_geo[key]\n",
    "                results.append((idx, longitude, latitude, \"exact_match\"))\n",
    "                match_method = \"exact_match\"\n",
    "                break\n",
    "        \n",
    "        if match_method:\n",
    "            continue\n",
    "        \n",
    "        # Try apartment name match\n",
    "        try:\n",
    "            if '아파트명' in chunk_df.columns and pd.notna(row['아파트명']):\n",
    "                apt_name = row['아파트명']\n",
    "                address_parts = row[first_col_name].split()\n",
    "                \n",
    "                if len(address_parts) >= 3:\n",
    "                    for addr in adres_to_geo.keys():\n",
    "                        # Check if both the neighborhood and apartment name match\n",
    "                        if address_parts[2] in addr and apt_name in addr:\n",
    "                            latitude, longitude = adres_to_geo[addr]\n",
    "                            results.append((idx, longitude, latitude, \"apartment_match\"))\n",
    "                            match_method = \"apartment_match\"\n",
    "                            break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if match_method:\n",
    "            continue\n",
    "        \n",
    "        # Try neighborhood match\n",
    "        try:\n",
    "            address_parts = row[first_col_name].split()\n",
    "            if len(address_parts) >= 3:\n",
    "                neighborhood = address_parts[2]  # e.g., \"개포동\"\n",
    "                \n",
    "                # Find any address in the same neighborhood\n",
    "                neighborhood_matches = [addr for addr in adres_to_geo.keys() if neighborhood in addr]\n",
    "                \n",
    "                if neighborhood_matches:\n",
    "                    # Use the first match\n",
    "                    latitude, longitude = adres_to_geo[neighborhood_matches[0]]\n",
    "                    results.append((idx, longitude, latitude, \"neighborhood_match\"))\n",
    "                    match_method = \"neighborhood_match\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If no match found, add to results with original NaN values\n",
    "        if not match_method:\n",
    "            results.append((idx, coord_x, coord_y, None))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def parallel_process_coordinates():\n",
    "    \"\"\"Process the dataset in parallel to fill missing coordinates\"\"\"\n",
    "    print(\"Starting parallel coordinate processing...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    df, adres_to_geo = load_data()\n",
    "    \n",
    "    # Determine the first column name\n",
    "    first_col_name = df.columns[0]\n",
    "    \n",
    "    # Check initial missing values\n",
    "    initial_missing = df[df['좌표X'].isna() | df['좌표Y'].isna()].shape[0]\n",
    "    print(f\"Initial missing coordinates: {initial_missing}\")\n",
    "    \n",
    "    # Identify rows with missing coordinates\n",
    "    missing_df = df[df['좌표X'].isna() | df['좌표Y'].isna()].copy()\n",
    "    \n",
    "    # Determine the number of CPU cores to use\n",
    "    num_cores = max(mp.cpu_count() - 1, 1)  # Leave one core free\n",
    "    print(f\"Using {num_cores} CPU cores for parallel processing\")\n",
    "    \n",
    "    # Split the missing data into chunks for parallel processing\n",
    "    chunk_size = max(1, len(missing_df) // (num_cores * 10))  # Create more chunks than cores\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(missing_df), chunk_size):\n",
    "        chunk = missing_df.iloc[i:i+chunk_size]\n",
    "        chunks.append((chunk, adres_to_geo, first_col_name))\n",
    "    \n",
    "    print(f\"Split data into {len(chunks)} chunks for processing\")\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with mp.Pool(num_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(process_chunk, chunks), total=len(chunks), desc=\"Processing chunks\"))\n",
    "    \n",
    "    # Flatten results\n",
    "    all_results = []\n",
    "    for chunk_result in results:\n",
    "        all_results.extend(chunk_result)\n",
    "    \n",
    "    # Count matches by method\n",
    "    match_counts = {\"exact_match\": 0, \"apartment_match\": 0, \"neighborhood_match\": 0}\n",
    "    filled_count = 0\n",
    "\n",
    "    # Before updating the dataframe:\n",
    "    print(f\"Aggregated {len(all_results)} results. Updating dataframe...\")\n",
    "    # Update the original dataframe with the results\n",
    "    for idx, coord_x, coord_y, match_method in all_results:\n",
    "        if match_method is not None:\n",
    "            df.loc[idx, '좌표X'] = coord_x\n",
    "            df.loc[idx, '좌표Y'] = coord_y\n",
    "            match_counts[match_method] += 1\n",
    "            filled_count += 1\n",
    "    \n",
    "    # Check how many coordinates were successfully filled\n",
    "    final_missing = df[df['좌표X'].isna() | df['좌표Y'].isna()].shape[0]\n",
    "    \n",
    "    print(f\"\\nProcessing completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully filled {filled_count} out of {initial_missing} missing coordinates\")\n",
    "    print(f\"Remaining missing coordinates: {final_missing}\")\n",
    "    print(f\"Match methods statistics:\")\n",
    "    for method, count in match_counts.items():\n",
    "        print(f\"  - {method}: {count}\")\n",
    "    \n",
    "\n",
    "    # Save the updated dataset\n",
    "    output_path = 'updated_test_with_coordinates_parallel.csv'\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Updated dataset saved to {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parallel_process_coordinates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
